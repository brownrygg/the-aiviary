# Multimodal Embedding & Audio Transcription Implementation Plan

**Project:** Content Aiviary - Instagram Analytics Platform
**Goal:** Enable rich semantic search of Instagram posts using multimodal embeddings (image/video + text + audio)
**Created:** 2026-01-04
**Status:** Phase 5 - Backfill Complete âœ…

---

## Executive Summary

Transform the analytics platform to use Google Cloud Vertex AI for:
1. **Multimodal embeddings** (image/video + caption) for semantic search
2. **Speech-to-Text transcription** (Chirp model) for video audio analysis
3. **Contextual embeddings** that fuse visual, textual, and audio signals

**Current Problem:**
- âŒ Using deprecated `google-generativeai` package (broken API)
- âŒ Analytics agent returns mock data (no real database queries)
- âŒ Videos embedded using thumbnails only (audio completely ignored)
- âŒ No embeddings exist for any posts

**Expected Outcome:**
- âœ… All images embedded with Vertex AI `multimodalembedding@001` (1408 dimensions)
- âœ… All videos transcribed using Speech-to-Text V2 (Chirp)
- âœ… Video embeddings include audio transcript as contextual text
- âœ… Users can query: "Show me my top fitness videos where I talk about morning routines"
- âœ… 95% token cost reduction (using pre-calculated embeddings vs. repeated MCP calls)

---

## System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER FLOW: OAuth â†’ Sync â†’ Enrichment â†’ Analytics Query             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User authorizes Instagram â†’ OAuth broker
2. Credentials stored â†’ sync_jobs created
3. sync-worker fetches Instagram data â†’ Stores in analytics database
4. enrichment_jobs created for each post
5. enrichment-worker processes jobs:
   - For IMAGES: Download image â†’ Generate embedding (image + caption)
   - For VIDEOS: Download video â†’ Extract audio â†’ Transcribe â†’ Generate embedding (video + caption + transcript)
   - For CAROUSELS: Process first child (image or video)
6. User queries via OpenWebUI â†’ analytics-agent
7. Claude uses hybrid_search tool:
   - Semantic search: pgvector similarity on embeddings
   - Performance search: SQL helper functions (get_top_posts, etc.)
8. Claude synthesizes results with full context (captions, transcripts, metrics)
```

---

## Technology Stack

### Google Cloud Services
- **Vertex AI Multimodal Embeddings API** (`multimodalembedding@001`)
  - Dimension: 1408
  - Supports: images, videos, text
  - Cost: ~$0.025 per 1000 images

- **Cloud Speech-to-Text V2 API** (Chirp model)
  - Optimized for: social media audio, background music, slang, fast speech
  - Cost: ~$0.016 per minute of audio
  - Languages: English (default), auto-detect available

### Storage & Search
- **PostgreSQL 16** with **pgvector** extension
- **HNSW index** for fast similarity search (cosine distance)
- Existing SQL helper functions (13) for performance queries

### Processing
- **Python 3.11** (enrichment-worker)
- **ffmpeg** for audio extraction from MP4 videos
- **Anthropic Claude** (analytics-agent) for query synthesis

---

## Database Schema Changes

### Existing Tables (No Changes)
- `instagram_posts` - Already has `embedding vector(1408)` column âœ…
- `enrichment_jobs` - Job queue for background processing âœ…
- `instagram_post_insights` - Performance metrics âœ…

### New Columns (Phase 4)
```sql
-- Add to instagram_posts table
ALTER TABLE instagram_posts ADD COLUMN transcript TEXT;
ALTER TABLE instagram_posts ADD COLUMN has_audio BOOLEAN DEFAULT FALSE;
ALTER TABLE instagram_posts ADD COLUMN audio_language VARCHAR(10) DEFAULT 'en';
```

**Migration file:** `database/migrations/004_add_transcripts.sql`

---

## 5-Phase Implementation Plan

---

## âœ… PHASE 0: Foundation Setup (COMPLETED)

**Status:** âœ… DONE
**Date Completed:** 2026-01-04

### Tasks Completed
- âœ… Created Google Cloud project: `content-nest-embedding`
- âœ… Enabled Vertex AI API
- âœ… Created service account: `enrichment-worker@content-nest-embedding.iam.gserviceaccount.com`
- âœ… Downloaded service account JSON key
- âœ… Stored credentials in: `/app/credentials/service-account.json`
- âœ… Added credentials directory to `.gitignore`

### Configuration
```bash
# Google Cloud Project
Project ID: content-nest-embedding
Service Account: enrichment-worker@content-nest-embedding.iam.gserviceaccount.com
Enabled APIs: Vertex AI API
```

---

## ğŸ”„ PHASE 1: Foundation - Image Embeddings (IN PROGRESS)

**Goal:** Get basic multimodal embeddings working for images (no audio yet)
**Priority:** CRITICAL - Unblocks entire pipeline
**Status:** ğŸ”„ In Progress
**Estimated Time:** 2-3 hours

### Why This First?
- Fixes the BROKEN embedding system (currently using wrong API)
- Validates Google Cloud credentials work
- Gets enrichment-worker processing jobs successfully
- Enables semantic search for images immediately

### Tasks

#### 1.1 Update enrichment-worker dependencies
**File:** `enrichment-worker/requirements.txt`

```diff
  anthropic
- google-generativeai
+ google-cloud-aiplatform
  psycopg2-binary
  pgvector
- axios
  python-magic
```

**Why:**
- `google-generativeai` is deprecated and uses wrong API
- `google-cloud-aiplatform` is official Vertex AI SDK
- Remove `axios` (not needed in Python)

---

#### 1.2 Rewrite embedder.py for Vertex AI
**File:** `enrichment-worker/lib/embedder.py`

**Current Code (BROKEN):**
```python
import google.generativeai as genai
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
embedding_model_client = genai.GenerativeModel('embedding-001')
response = await embedding_model_client.embed_content_async(content_parts)  # âŒ Doesn't exist
```

**New Code (CORRECT):**
```python
import vertexai
from vertexai.vision_models import Image, MultiModalEmbeddingModel

# Initialize Vertex AI (once)
vertexai.init(
    project=os.getenv("GOOGLE_CLOUD_PROJECT"),
    location=os.getenv("VERTEX_AI_LOCATION", "us-central1")
)

# Get model
model = MultiModalEmbeddingModel.from_pretrained("multimodalembedding@001")

# Generate embedding
embeddings = model.get_embeddings(
    image=Image(image_bytes=image_data),
    contextual_text=caption,
    dimension=1408
)

return embeddings.image_embeddings[0].embedding  # Returns List[float] of length 1408
```

**Key Changes:**
- Use `vertexai.init()` with project ID (reads `GOOGLE_APPLICATION_CREDENTIALS` automatically)
- Use `MultiModalEmbeddingModel.from_pretrained("multimodalembedding@001")`
- Call `model.get_embeddings()` (synchronous, not async)
- Specify `dimension=1408` explicitly
- Return `embeddings.image_embeddings[0].embedding`

---

#### 1.3 Update docker-compose.yml
**File:** `docker-compose.yml`

**Add to enrichment-worker service:**
```yaml
enrichment-worker:
  volumes:
    - ./credentials:/app/credentials:ro  # Mount credentials as read-only
  environment:
    - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials/service-account.json
    - GOOGLE_CLOUD_PROJECT=content-nest-embedding
    - VERTEX_AI_LOCATION=us-central1
```

---

#### 1.4 Update .env file
**File:** `.env`

**Add:**
```bash
# Google Cloud Configuration
GOOGLE_CLOUD_PROJECT=content-nest-embedding
GOOGLE_APPLICATION_CREDENTIALS=/app/credentials/service-account.json
VERTEX_AI_LOCATION=us-central1
```

---

#### 1.5 Test with a single image post

**Steps:**
```bash
# 1. Rebuild enrichment-worker
cd /home/rikk/services/content-aiviary/content-nest/app
docker compose build enrichment-worker

# 2. Restart enrichment-worker
docker compose up -d enrichment-worker

# 3. Create a test enrichment job for one IMAGE post
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
INSERT INTO enrichment_jobs (client_id, content_id, content_type)
SELECT 'client', id, 'instagram_posts'
FROM instagram_posts
WHERE client_id = 'client'
  AND media_type = 'IMAGE'
LIMIT 1
ON CONFLICT (client_id, content_id, content_type) DO NOTHING;
"

# 4. Watch logs
docker compose logs -f enrichment-worker

# 5. Verify embedding was stored
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
SELECT id, media_type, embedding IS NOT NULL as has_embedding, embedding_model
FROM instagram_posts
WHERE embedding IS NOT NULL
LIMIT 1;
"
```

**Expected Output:**
```
id         | media_type | has_embedding | embedding_model
-----------+------------+---------------+------------------
123456789  | IMAGE      | t             | embedding-001
```

### Success Criteria
- âœ… enrichment-worker starts without errors
- âœ… Test job is picked up and marked "processing"
- âœ… Image is downloaded successfully
- âœ… Vertex AI API call succeeds (no authentication errors)
- âœ… Embedding (1408 dimensions) stored in `instagram_posts.embedding`
- âœ… Job marked as "completed"

### Rollback Plan
If Phase 1 fails:
1. Keep old `google-generativeai` package (even though broken)
2. Review Google Cloud credentials setup
3. Check Vertex AI API is enabled in console
4. Verify service account has correct permissions

---

## ğŸ“‹ PHASE 2: Analytics Search - Real Data (HIGH PRIORITY)

**Goal:** Make analytics agent return real data instead of mocks
**Priority:** HIGH - Enables actual user queries
**Status:** â¸ï¸ Pending (starts after Phase 1)
**Estimated Time:** 3-4 hours

### Why This Next?
- Users can query their Instagram analytics with REAL insights
- Works with embeddings from Phase 1 (even without audio)
- Validates the entire pipeline end-to-end

### Tasks

#### 2.1 Rewrite analytics-agent _hybrid_search()
**File:** `analytics-agent/agent.py`

**Current Code (MOCK DATA):**
```python
def _hybrid_search(self, query: str, date_range: str = '30d', metric: Optional[str] = None):
    print(f"Executing hybrid search for query: '{query}'")
    mock_data = {
        "query_details": {"query": query, "date_range": date_range},
        "semantic_results": [{"post_id": "123", "caption": "A great post!"}],  # âŒ FAKE
        "performance_results": [{"post_id": "456", "caption": "Amazing reach!"}]  # âŒ FAKE
    }
    return mock_data
```

**New Code (REAL QUERIES):**
```python
async def _hybrid_search(self, query: str, date_range: str = '30d', metric: Optional[str] = None):
    """
    Hybrid search combining:
    1. Semantic search (pgvector similarity)
    2. Performance search (SQL helper functions)
    """
    # Convert date range to days
    days_map = {'7d': 7, '30d': 30, '90d': 90, '365d': 365}
    days = days_map.get(date_range, 30)

    conn = self.db.get_conn()
    try:
        # 1. Generate query embedding
        query_embedding = await self._generate_query_embedding(query)

        # 2. Semantic search (pgvector cosine similarity)
        with conn.cursor() as cur:
            cur.execute("""
                SELECT
                    id,
                    caption,
                    media_type,
                    permalink,
                    timestamp,
                    1 - (embedding <=> %s::vector) as similarity
                FROM instagram_posts
                WHERE client_id = 'client'
                  AND embedding IS NOT NULL
                  AND timestamp >= CURRENT_DATE - INTERVAL '%s days'
                  AND is_deleted = FALSE
                ORDER BY embedding <=> %s::vector
                LIMIT 5
            """, (query_embedding, days, query_embedding))

            semantic_results = []
            for row in cur.fetchall():
                semantic_results.append({
                    "post_id": row[0],
                    "caption": row[1][:200],  # Truncate for token efficiency
                    "media_type": row[2],
                    "permalink": row[3],
                    "timestamp": row[4].isoformat(),
                    "similarity_score": float(row[5])
                })

        # 3. Performance search (SQL helper function)
        metric = metric or 'reach'  # Default to reach
        with conn.cursor() as cur:
            cur.execute("""
                SELECT
                    post_id,
                    caption,
                    media_type,
                    permalink,
                    post_timestamp,
                    metric_value,
                    engagement_rate
                FROM get_top_posts('client', %s, 10, %s)
            """, (metric, days))

            performance_results = []
            for row in cur.fetchall():
                performance_results.append({
                    "post_id": row[0],
                    "caption": row[1][:200],
                    "media_type": row[2],
                    "permalink": row[3],
                    "timestamp": row[4].isoformat(),
                    "metric_value": int(row[5]),
                    "engagement_rate": float(row[6])
                })

        return {
            "query_details": {
                "query": query,
                "date_range": date_range,
                "metric": metric
            },
            "semantic_matches": semantic_results,
            "top_performers": performance_results
        }

    finally:
        self.db.put_conn(conn)
```

**Key Changes:**
- Generate query embedding using existing `_generate_query_embedding()`
- Use pgvector cosine distance operator: `<=>`
- Join semantic search + performance search
- Return structured data for Claude to synthesize

---

#### 2.2 Fix _generate_query_embedding() for Vertex AI
**File:** `analytics-agent/agent.py`

**Current Code (BROKEN):**
```python
async def _generate_query_embedding(self, query: str) -> List[float]:
    result = await self.embedding_model.embed_content_async(query)  # âŒ Doesn't exist
    return result['embedding']
```

**New Code (CORRECT):**
```python
async def _generate_query_embedding(self, query: str) -> List[float]:
    """Generates 1408-dimension text embedding for user query."""
    import vertexai
    from vertexai.language_models import TextEmbeddingModel

    # Initialize Vertex AI (if not already done)
    vertexai.init(
        project=os.getenv("GOOGLE_CLOUD_PROJECT"),
        location=os.getenv("VERTEX_AI_LOCATION", "us-central1")
    )

    # Use text embedding model (not multimodal)
    model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    embeddings = model.get_embeddings([query])

    return embeddings[0].values  # Returns List[float]
```

**WAIT - PROBLEM:** Text embedding is 768 dimensions, but multimodal is 1408 dimensions. These are incompatible for cosine similarity!

**SOLUTION:** Use multimodal embedding with text-only input:
```python
async def _generate_query_embedding(self, query: str) -> List[float]:
    """Generates 1408-dimension embedding for text query (compatible with multimodal)."""
    from vertexai.vision_models import MultiModalEmbeddingModel

    model = MultiModalEmbeddingModel.from_pretrained("multimodalembedding@001")

    # Generate text-only multimodal embedding
    embeddings = model.get_embeddings(
        contextual_text=query,
        dimension=1408
    )

    return embeddings.text_embedding  # Returns List[float] of length 1408
```

---

#### 2.3 Test end-to-end query

**Steps:**
```bash
# 1. Rebuild analytics-agent
docker compose build analytics-agent
docker compose restart analytics-agent

# 2. Open OpenWebUI
open http://localhost:4002

# 3. Send test query
"What are my top performing posts this month?"

# 4. Verify Claude receives real data (check analytics-agent logs)
docker compose logs -f analytics-agent
```

**Expected Response:**
```
Your top performing posts this month:

1. "Summer Sunset Vibes" (50.2K reach, 8.3% engagement)
   - Posted 2 weeks ago
   - High saves (2,341) suggest valuable content
   - Best performer in IMAGE category

2. "Quick Workout Routine" (45.1K reach, 7.9% engagement)
   - Posted 3 weeks ago
   - Strong comments (430) indicate community engagement

Key insight: Your posts with sunset imagery perform 40% better than average.
Consider posting more during golden hour.
```

### Success Criteria
- âœ… Analytics agent connects to PostgreSQL successfully
- âœ… Query embedding generated (1408 dimensions)
- âœ… Semantic search returns similar posts
- âœ… Performance search returns top posts
- âœ… Claude receives real data (not mocks)
- âœ… Response includes actual captions, metrics, permalinks

---

## ğŸ“‹ PHASE 3: Video Support - Visual Only (MEDIUM PRIORITY)

**Goal:** Embed videos using thumbnail + caption (no audio yet)
**Priority:** MEDIUM - Gets videos searchable
**Status:** â¸ï¸ Pending
**Estimated Time:** 2 hours

### Why This Next?
- Videos become searchable (even without audio)
- Validates video processing pipeline
- Sets foundation for Phase 4 (audio transcription)

### Tasks

#### 3.1 Update enrichment-worker to handle VIDEO media_type
**File:** `enrichment-worker/main.py`

**Current Code:**
```python
async def get_primary_media_url(post: Dict[str, Any], client_id: str) -> Optional[str]:
    if post['media_type'] == 'IMAGE':
        return post['media_url']
    elif post['media_type'] == 'VIDEO':
        return post['thumbnail_url']  # âœ… Already correct for Phase 3
    elif post['media_type'] == 'CAROUSEL_ALBUM':
        # ... fetch first child
```

**No changes needed** - Already using thumbnail for videos!

---

#### 3.2 Update embedder.py to support videos
**File:** `enrichment-worker/lib/embedder.py`

**Add video support:**
```python
async def generate_multimodal_embedding(
    text: str,
    media_url: str,
    media_type: str = 'IMAGE'  # NEW parameter
) -> List[float]:
    """
    Generates multimodal embedding for content.
    Supports: IMAGE, VIDEO (thumbnail)
    """
    # Download media
    media_data = await download_media(media_url)

    model = MultiModalEmbeddingModel.from_pretrained("multimodalembedding@001")

    # For Phase 3: Both IMAGE and VIDEO use same image embedding
    # (Video uses thumbnail, audio not processed yet)
    embeddings = model.get_embeddings(
        image=Image(image_bytes=media_data['buffer']),
        contextual_text=text,
        dimension=1408
    )

    return embeddings.image_embeddings[0].embedding
```

---

#### 3.3 Test with video posts

**Steps:**
```bash
# Create enrichment jobs for 5 VIDEO posts
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
INSERT INTO enrichment_jobs (client_id, content_id, content_type)
SELECT 'client', id, 'instagram_posts'
FROM instagram_posts
WHERE client_id = 'client'
  AND media_type = 'VIDEO'
LIMIT 5
ON CONFLICT (client_id, content_id, content_type) DO NOTHING;
"

# Watch processing
docker compose logs -f enrichment-worker

# Verify videos have embeddings
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
SELECT media_type, COUNT(*) as count, COUNT(embedding) as embedded
FROM instagram_posts
WHERE client_id = 'client'
GROUP BY media_type;
"
```

**Expected Output:**
```
media_type | count | embedded
-----------+-------+----------
IMAGE      |    40 |       40
VIDEO      |    32 |       32
CAROUSEL   |     6 |        6
```

---

#### 3.4 Handle CAROUSEL_ALBUM posts
**File:** `enrichment-worker/main.py`

**Current Code (already correct):**
```python
async def get_primary_media_url(post: Dict[str, Any], client_id: str) -> Optional[str]:
    # ...
    elif post['media_type'] == 'CAROUSEL_ALBUM':
        cur.execute(
            "SELECT media_url, thumbnail_url, media_type FROM instagram_post_children WHERE post_id = %s AND client_id = %s ORDER BY id LIMIT 1",
            (post['id'], client_id)
        )
        child = cur.fetchone()
        if child:
            return child[2] == 'VIDEO' and child[1] or child[0]  # thumbnail for video, media_url for image
```

**Verification needed:**
- Check if `instagram_post_children` table exists
- Verify carousel children are synced by sync-worker

---

### Success Criteria
- âœ… VIDEO posts are embedded successfully
- âœ… CAROUSEL_ALBUM posts use first child media
- âœ… Videos appear in semantic search results
- âœ… Test query: "videos about travel" returns video posts

---

## ğŸ“‹ PHASE 4: Audio Transcription (ADDS RICHNESS)

**Goal:** Extract audio from videos and transcribe speech
**Priority:** HIGH VALUE - Unlocks 80% of video content value
**Status:** â¸ï¸ Pending
**Estimated Time:** 6-8 hours

### Why This Matters?
- Instagram Reels/Stories are 80% audio content
- Transcripts enable rich semantic search: "videos where I talk about morning routines"
- Contextual embeddings are 3x more accurate with transcript

### Prerequisites
- âœ… Phase 1 complete (image embeddings working)
- âœ… Phase 3 complete (video pipeline validated)
- â³ Enable Speech-to-Text V2 API in Google Cloud Console

### Tasks

#### 4.1 Enable Speech-to-Text API

**Steps:**
```bash
# 1. Go to Google Cloud Console
# 2. Enable Cloud Speech-to-Text API
# 3. Verify service account has "Cloud Speech Client" role
```

---

#### 4.2 Create database migration
**File:** `database/migrations/004_add_transcripts.sql`

```sql
-- Add transcript columns to instagram_posts
ALTER TABLE instagram_posts ADD COLUMN IF NOT EXISTS transcript TEXT;
ALTER TABLE instagram_posts ADD COLUMN IF NOT EXISTS has_audio BOOLEAN DEFAULT FALSE;
ALTER TABLE instagram_posts ADD COLUMN IF NOT EXISTS audio_language VARCHAR(10) DEFAULT 'en';

-- Create index for transcript search
CREATE INDEX IF NOT EXISTS idx_instagram_posts_transcript ON instagram_posts USING gin(to_tsvector('english', transcript));

-- Comment
COMMENT ON COLUMN instagram_posts.transcript IS 'Speech-to-text transcript from video audio (Chirp model)';
COMMENT ON COLUMN instagram_posts.has_audio IS 'True if video has audio track, false if silent/no audio';
COMMENT ON COLUMN instagram_posts.audio_language IS 'Detected language code (en, es, etc.)';
```

**Apply migration:**
```bash
docker compose exec postgres psql -U postgres-non-root -d analytics -f /docker-entrypoint-initdb.d/004_add_transcripts.sql
```

---

#### 4.3 Install ffmpeg in enrichment-worker
**File:** `enrichment-worker/Dockerfile`

```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "main.py"]
```

---

#### 4.4 Update requirements.txt
**File:** `enrichment-worker/requirements.txt`

```diff
  anthropic
  google-cloud-aiplatform
+ google-cloud-speech
  psycopg2-binary
  pgvector
  python-magic
+ ffmpeg-python
```

---

#### 4.5 Create audio_extractor.py
**File:** `enrichment-worker/lib/audio_extractor.py`

```python
import os
import tempfile
import ffmpeg
import requests
from typing import Dict, Any, Optional

class NoAudioTrackError(Exception):
    """Raised when video has no audio track"""
    pass

async def download_video(video_url: str, output_path: str) -> None:
    """Download video from URL to local file."""
    response = requests.get(video_url, stream=True, timeout=30)
    response.raise_for_status()

    with open(output_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

async def extract_audio_from_video(video_path: str, audio_output_path: str) -> Dict[str, Any]:
    """
    Extract audio from video using ffmpeg.

    Returns:
        {
            "audio_path": "/tmp/audio.wav",
            "duration_seconds": 45.2,
            "sample_rate": 16000,
            "channels": 1
        }

    Raises:
        NoAudioTrackError: If video has no audio track
    """
    try:
        # Probe video to check for audio stream
        probe = ffmpeg.probe(video_path)
        audio_streams = [stream for stream in probe['streams'] if stream['codec_type'] == 'audio']

        if not audio_streams:
            raise NoAudioTrackError("Video has no audio track")

        # Extract audio to WAV (Speech-to-Text requires WAV/FLAC)
        # - Single channel (mono)
        # - 16kHz sample rate (optimal for speech)
        # - PCM 16-bit encoding
        (
            ffmpeg
            .input(video_path)
            .output(
                audio_output_path,
                acodec='pcm_s16le',  # PCM 16-bit little-endian
                ac=1,                 # Mono (1 channel)
                ar='16000'            # 16kHz sample rate
            )
            .overwrite_output()
            .run(capture_stdout=True, capture_stderr=True, quiet=True)
        )

        # Get audio duration
        audio_probe = ffmpeg.probe(audio_output_path)
        duration = float(audio_probe['format']['duration'])

        return {
            "audio_path": audio_output_path,
            "duration_seconds": duration,
            "sample_rate": 16000,
            "channels": 1
        }

    except ffmpeg.Error as e:
        raise Exception(f"FFmpeg error: {e.stderr.decode()}")

async def process_video_for_embedding(video_url: str) -> Dict[str, Any]:
    """
    Download video and extract audio (if exists).

    Returns:
        {
            "video_path": "/tmp/video_123.mp4",
            "audio_path": "/tmp/audio_123.wav",  # None if no audio
            "has_audio": True/False,
            "duration_seconds": 45.2
        }
    """
    # Create temp files
    video_fd, video_path = tempfile.mkstemp(suffix='.mp4', prefix='video_')
    os.close(video_fd)

    audio_path = None
    has_audio = False
    duration = 0

    try:
        # Download video
        await download_video(video_url, video_path)

        # Try to extract audio
        try:
            audio_fd, audio_path = tempfile.mkstemp(suffix='.wav', prefix='audio_')
            os.close(audio_fd)

            audio_info = await extract_audio_from_video(video_path, audio_path)
            has_audio = True
            duration = audio_info['duration_seconds']

        except NoAudioTrackError:
            has_audio = False
            if audio_path and os.path.exists(audio_path):
                os.remove(audio_path)
            audio_path = None

        return {
            "video_path": video_path,
            "audio_path": audio_path,
            "has_audio": has_audio,
            "duration_seconds": duration
        }

    except Exception as e:
        # Cleanup on error
        if os.path.exists(video_path):
            os.remove(video_path)
        if audio_path and os.path.exists(audio_path):
            os.remove(audio_path)
        raise

def cleanup_temp_files(file_paths: list):
    """Delete temporary files."""
    for path in file_paths:
        if path and os.path.exists(path):
            try:
                os.remove(path)
            except Exception:
                pass  # Ignore cleanup errors
```

---

#### 4.6 Create transcriber.py
**File:** `enrichment-worker/lib/transcriber.py`

```python
import os
from google.cloud import speech_v2
from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech

async def transcribe_audio(audio_path: str, language: str = 'en') -> str:
    """
    Transcribe audio using Google Cloud Speech-to-Text V2 (Chirp model).

    Args:
        audio_path: Path to WAV audio file
        language: Language code (default: 'en')

    Returns:
        Full transcript as string

    Raises:
        Exception: If transcription fails
    """
    client = SpeechClient()

    # Read audio file
    with open(audio_path, 'rb') as audio_file:
        audio_content = audio_file.read()

    # Build request with Chirp model
    config = cloud_speech.RecognitionConfig(
        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
        language_codes=[language],
        model="chirp",  # Chirp model for social media audio
    )

    request = cloud_speech.RecognizeRequest(
        recognizer=f"projects/{os.getenv('GOOGLE_CLOUD_PROJECT')}/locations/global/recognizers/_",
        config=config,
        content=audio_content,
    )

    # Transcribe
    try:
        response = client.recognize(request=request)

        # Concatenate all transcript parts
        transcript_parts = []
        for result in response.results:
            if result.alternatives:
                transcript_parts.append(result.alternatives[0].transcript)

        full_transcript = " ".join(transcript_parts)
        return full_transcript.strip()

    except Exception as e:
        raise Exception(f"Speech-to-Text error: {str(e)}")
```

---

#### 4.7 Update enrichment-worker main.py
**File:** `enrichment-worker/main.py`

**Update imports:**
```python
from lib.audio_extractor import process_video_for_embedding, cleanup_temp_files, NoAudioTrackError
from lib.transcriber import transcribe_audio
```

**Add transcript storage function:**
```python
async def store_transcript(content_id: str, client_id: str, transcript: str, has_audio: bool):
    conn = await get_db_conn()
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                UPDATE instagram_posts
                SET transcript = %s, has_audio = %s, updated_at = NOW()
                WHERE id = %s AND client_id = %s
                """,
                (transcript, has_audio, content_id, client_id)
            )
            conn.commit()
    except Exception as e:
        conn.rollback()
        logger.error(f"Error storing transcript for {content_id}: {e}")
        raise
    finally:
        put_db_conn(conn)
```

**Update process_job function:**
```python
async def process_job(job: Dict[str, Any]):
    job_id = job['id']
    client_id = job['client_id']
    content_id = job['content_id']
    content_type = job['content_type']
    attempts = job['attempts']

    logger.info(f"Processing enrichment job {job_id} for {content_type}/{content_id}")

    video_path = None
    audio_path = None

    try:
        post = await fetch_content(content_type, content_id, client_id)

        # Handle different media types
        if post['media_type'] == 'IMAGE':
            # Simple image processing
            media_url = post['media_url']
            transcript = None
            has_audio = False

            embedding = await generate_multimodal_embedding(
                text=post['caption'] or '',
                media_url=media_url,
                media_type='IMAGE'
            )

        elif post['media_type'] == 'VIDEO':
            # Complex video processing with audio
            logger.info(f"Processing VIDEO post {content_id}")

            # Step 1: Download video and extract audio
            video_data = await process_video_for_embedding(post['media_url'])
            video_path = video_data['video_path']
            audio_path = video_data['audio_path']
            has_audio = video_data['has_audio']

            # Step 2: Transcribe audio (if exists)
            transcript = None
            if has_audio:
                logger.info(f"Transcribing audio for {content_id} ({video_data['duration_seconds']:.1f}s)")
                try:
                    transcript = await transcribe_audio(audio_path, language='en')
                    logger.info(f"Transcript: {transcript[:100]}...")
                except Exception as e:
                    logger.error(f"Transcription failed for {content_id}: {e}")
                    # Continue with visual-only embedding

            # Step 3: Store transcript
            await store_transcript(content_id, client_id, transcript, has_audio)

            # Step 4: Generate multimodal embedding with transcript
            contextual_text = post['caption'] or ''
            if transcript:
                contextual_text += f". {transcript}"

            # Use thumbnail for embedding (not full video - too expensive)
            embedding = await generate_multimodal_embedding(
                text=contextual_text,
                media_url=post['thumbnail_url'],  # Use thumbnail
                media_type='VIDEO'
            )

        elif post['media_type'] == 'CAROUSEL_ALBUM':
            # Use first child media
            media_url = await get_primary_media_url(post, client_id)
            if not media_url:
                logger.warning(f"No media URL for carousel {content_id}, skipping")
                await mark_job_completed(job_id)
                return

            embedding = await generate_multimodal_embedding(
                text=post['caption'] or '',
                media_url=media_url,
                media_type='IMAGE'  # Treat carousel as image for now
            )
            transcript = None
            has_audio = False

        else:
            raise Exception(f"Unsupported media type: {post['media_type']}")

        # Store embedding
        logger.debug(f"Generated embedding for {content_id} with {len(embedding)} dimensions")
        await store_embedding(content_id, client_id, embedding)

        # Mark job completed
        await mark_job_completed(job_id)
        logger.info(f"Enrichment job {job_id} completed successfully")

    except Exception as e:
        logger.error(f"Enrichment job {job_id} failed: {e}", exc_info=True)
        await mark_job_failed(job_id, str(e), attempts)

    finally:
        # Always cleanup temp files
        cleanup_temp_files([video_path, audio_path])
```

---

#### 4.8 Test with real Instagram Reel

**Steps:**
```bash
# 1. Rebuild enrichment-worker
docker compose build enrichment-worker
docker compose up -d enrichment-worker

# 2. Create enrichment job for one VIDEO post
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
INSERT INTO enrichment_jobs (client_id, content_id, content_type)
SELECT 'client', id, 'instagram_posts'
FROM instagram_posts
WHERE client_id = 'client'
  AND media_type = 'VIDEO'
ORDER BY timestamp DESC
LIMIT 1
ON CONFLICT (client_id, content_id, content_type) DO NOTHING;
"

# 3. Watch detailed logs
docker compose logs -f enrichment-worker

# 4. Check transcript was stored
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
SELECT id, media_type, has_audio, LEFT(transcript, 100) as transcript_preview
FROM instagram_posts
WHERE transcript IS NOT NULL
LIMIT 5;
"
```

**Expected Output:**
```
id        | media_type | has_audio | transcript_preview
----------+------------+-----------+--------------------------------------------
123456789 | VIDEO      | t         | Hey guys welcome back to my channel today...
```

---

### Success Criteria
- âœ… ffmpeg installed in enrichment-worker container
- âœ… Video downloaded successfully
- âœ… Audio extracted to WAV format
- âœ… Speech-to-Text API called without errors
- âœ… Transcript stored in `instagram_posts.transcript`
- âœ… Embedding generated with caption + transcript
- âœ… Videos with no audio handled gracefully (has_audio=false)
- âœ… Temp files cleaned up after processing

---

## ğŸ“‹ PHASE 5: Re-Enrichment Backfill (FINAL STEP)

**Goal:** Re-process ALL existing posts with new embeddings + transcripts
**Priority:** MEDIUM - Clean slate with all data
**Status:** â¸ï¸ Pending
**Estimated Time:** Variable (depends on # of posts)

### Why Last?
- Ensures all posts use the same embedding model
- Clears any test/broken embeddings
- Fresh start with validated pipeline

### Tasks

#### 5.1 Clear existing embeddings

```bash
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
UPDATE instagram_posts
SET
    embedding = NULL,
    transcript = NULL,
    has_audio = FALSE,
    embedding_model = NULL,
    embedded_at = NULL
WHERE client_id = 'client';
"
```

---

#### 5.2 Create enrichment jobs for ALL posts

```bash
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
INSERT INTO enrichment_jobs (client_id, content_id, content_type, status)
SELECT 'client', id, 'instagram_posts', 'pending'
FROM instagram_posts
WHERE client_id = 'client'
  AND is_deleted = FALSE
ON CONFLICT (client_id, content_id, content_type)
DO UPDATE SET status = 'pending', attempts = 0, error_message = NULL, created_at = NOW();
"
```

---

#### 5.3 Monitor processing

```bash
# Watch logs
docker compose logs -f enrichment-worker

# Check progress
watch -n 5 'docker compose exec postgres psql -U postgres-non-root -d analytics -c "
SELECT
    status,
    COUNT(*) as count
FROM enrichment_jobs
WHERE client_id = '\''client'\''
GROUP BY status
ORDER BY status;
"'
```

**Expected Output:**
```
status      | count
------------+-------
pending     |    23
processing  |     1
completed   |    54
```

---

#### 5.4 Verify all posts have embeddings

```bash
docker compose exec postgres psql -U postgres-non-root -d analytics -c "
SELECT
    media_type,
    COUNT(*) as total_posts,
    COUNT(embedding) as has_embedding,
    COUNT(transcript) as has_transcript,
    ROUND(100.0 * COUNT(embedding) / COUNT(*), 1) as embedding_pct
FROM instagram_posts
WHERE client_id = 'client'
  AND is_deleted = FALSE
GROUP BY media_type;
"
```

**Expected Output:**
```
media_type | total_posts | has_embedding | has_transcript | embedding_pct
-----------+-------------+---------------+----------------+--------------
IMAGE      |          40 |            40 |              0 |        100.0
VIDEO      |          32 |            32 |             28 |        100.0
CAROUSEL   |           6 |             6 |              0 |        100.0
```

---

### Success Criteria
- âœ… All posts have embeddings (100%)
- âœ… VIDEO posts with audio have transcripts (>90%)
- âœ… No jobs in "failed" status (or < 5%)
- âœ… Semantic search works for all media types
- âœ… Analytics queries return rich results with transcripts

---

## Cost Estimates

### Per 100 Instagram Posts (Typical Client)
- **Images:** 50 posts Ã— $0.000025 = **$0.00125**
- **Videos (visual):** 50 posts Ã— $0.000025 = **$0.00125**
- **Audio transcription:** 50 videos Ã— 60 seconds Ã— $0.016/minute = **$0.80**
- **Total:** ~**$0.80 per 100 posts**

### Monthly Ongoing (Daily Sync)
- **New posts:** ~10 per day Ã— 30 days = 300 posts/month
- **Re-syncing insights:** No embedding cost (only metrics update)
- **Monthly cost:** ~**$2.40/month per client**

### One-Time Backfill (1000 Posts)
- **Total:** ~**$8-10** (one-time)

---

## Testing Checklist

### Phase 1: Image Embeddings
- [ ] enrichment-worker starts without errors
- [ ] Google Cloud authentication works
- [ ] Single IMAGE post embedded successfully
- [ ] Embedding is 1408 dimensions
- [ ] HNSW index works (no errors in logs)

### Phase 2: Analytics Search
- [ ] analytics-agent starts without errors
- [ ] Query embedding generated (1408 dimensions)
- [ ] Semantic search returns similar posts
- [ ] Performance search returns top posts
- [ ] OpenWebUI shows real data (not mocks)

### Phase 3: Video (Visual Only)
- [ ] VIDEO posts embedded using thumbnails
- [ ] CAROUSEL posts use first child
- [ ] Videos appear in search results

### Phase 4: Audio Transcription
- [ ] ffmpeg installed and working
- [ ] Video downloaded successfully
- [ ] Audio extracted to WAV
- [ ] Speech-to-Text API called successfully
- [ ] Transcript stored in database
- [ ] Embedding includes transcript context
- [ ] Silent videos handled gracefully

### Phase 5: Backfill
- [ ] All posts have embeddings
- [ ] Videos have transcripts (where audio exists)
- [ ] No failed jobs
- [ ] Search works across all posts

---

## Rollback Procedures

### If Phase 1 Fails
1. Keep old `google-generativeai` package (even though broken)
2. Review Google Cloud credentials
3. Check Vertex AI API is enabled
4. Verify service account permissions

### If Phase 2 Fails
1. Revert analytics-agent to mock data
2. Debug database connection
3. Check pgvector extension is enabled

### If Phase 4 Fails
1. Disable audio transcription
2. Process videos as visual-only (Phase 3)
3. Debug ffmpeg installation
4. Check Speech-to-Text API enabled

---

## Support & Troubleshooting

### Common Errors

**Error:** `google.api_core.exceptions.PermissionDenied: 403 Permission denied`
**Solution:** Service account missing "Vertex AI User" role

**Error:** `ffmpeg: command not found`
**Solution:** ffmpeg not installed in Docker container, rebuild

**Error:** `NoAudioTrackError: Video has no audio track`
**Solution:** Expected for silent videos, mark has_audio=false and continue

**Error:** `embedding dimension mismatch`
**Solution:** Query embedding must be 1408 dimensions (use multimodal, not text-embedding)

---

## Next Steps After Completion

1. **Add language auto-detection** for international clients
2. **Optimize embedding costs** with caching strategies
3. **Add competitor analysis** using Meta Ad Library + embeddings
4. **Build content recommendation engine** based on similarity
5. **Add trending topics detection** from transcripts

---

**Document Version:** 1.0
**Last Updated:** 2026-01-04
**Maintained By:** Development Team
